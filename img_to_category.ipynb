{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a6420261f825>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file structure is ./data/small/[clothing_category]/[image no.]\n",
    "\n",
    "train_directory = './data/small/'\n",
    "# Form list of training images names\n",
    "directory_list  = os.listdir(train_directory)\n",
    "# Convert to 224 x 224 images\n",
    "\n",
    "ims = []\n",
    "\n",
    "for directory in directory_list:\n",
    "    # list all the images in the directory\n",
    "    images_list = os.listdir(os.path.join(train_directory, directory))\n",
    "    \n",
    "    # open the images and resize\n",
    "    ims.extend([np.array(Image.open(os.path.join(train_directory, directory, im)), dtype=np.float64).resize((224,224)) for im in images_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Blouse': 15,\n",
       "         'Cardigan': 15,\n",
       "         'Dress': 35,\n",
       "         'Hoodie': 5,\n",
       "         'Jacket': 9,\n",
       "         'Jeans': 9,\n",
       "         'Joggers': 6,\n",
       "         'Kimono': 8,\n",
       "         'Leggings': 8,\n",
       "         'Romper': 9,\n",
       "         'Shorts': 15,\n",
       "         'Skirt': 12,\n",
       "         'Sweater': 11,\n",
       "         'Sweatpants': 6,\n",
       "         'Tank': 13,\n",
       "         'Tee': 26,\n",
       "         'Top': 9})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "IGNORE\n",
    "d = './data/medium/'\n",
    "dlist = os.listdir(d)\n",
    "i = []\n",
    "for dl in dlist:\n",
    "    c = get_category(dl)\n",
    "    i.append(c)\n",
    "    \n",
    "    \n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for word in i:\n",
    "    cnt[word] += 1\n",
    "\n",
    "cnt\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_category(string):\n",
    "    # split by underscore\n",
    "    # get last element\n",
    "    temp = string.split(\"_\")\n",
    "    return temp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labs = []\n",
    "for direc in directory_list:\n",
    "    category = get_category(direc)\n",
    "    \n",
    "    images_list = os.listdir(os.path.join(train_directory, direc))\n",
    "\n",
    "    for i in range(len(images_list)):\n",
    "        labs.append(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now: All the images stored under \"ims\" and all the simplified labels stored under \"labs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "\n",
    "s = pd.Series(labs)\n",
    "one_hot_categories = pd.get_dummies(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "imlist = np.array([np.array(im, dtype=np.float64) for im in ims])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IM_HEIGHT = 224\n",
    "IM_WIDTH = 224\n",
    "NB_EPOCHS = 1\n",
    "BAT_SIZE = 16\n",
    "FC_SIZE = 500 # May need to train this parameter\n",
    "\n",
    "nb_classes = len(set(labs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_to_transfer_learn(model, base_model):\n",
    "    \"\"\"Freeze all layers and compile the model\"\"\"\n",
    "    adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        model.compile(optimizer='adam',    \n",
    "                    loss='categorical_crossentropy', \n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_new_last_layer(base_model, nb_classes):\n",
    "    \"\"\"Add last layer to the convnet\n",
    "    Args:\n",
    "    base_model: keras model excluding top\n",
    "    nb_classes: # of classes\n",
    "    Returns:\n",
    "    new keras model with last layer\n",
    "    \"\"\"\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(FC_SIZE, activation='relu')(x) #new FC layer, random init\n",
    "    predictions = Dense(nb_classes, activation='softmax')(x) #new softmax layer\n",
    "    model = Model(inputs = base_model.input, outputs = predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet_train(images, labels):\n",
    "    \n",
    "    base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "    model = add_new_last_layer(base_model, nb_classes)\n",
    "\n",
    "    setup_to_transfer_learn(model, base_model)\n",
    "\n",
    "    history = model.fit(images, labels)\n",
    "    model.save(\"resnet.h5\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "h = tqdm(resnet_train(imlist, one_hot_categories.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict - Load in trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_softmax(model):\n",
    "    model.layers.pop() # Get rid of the classification layer\n",
    "    model.outputs = [model.layers[-1].output]\n",
    "    model.layers[-1].outbound_nodes = []\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cnn_output(model, ims):\n",
    "    # ims is np array\n",
    "    if len(ims.shape) == 1:\n",
    "        ims = ims.reshape(1, -1)\n",
    "    \n",
    "    return (model.predict(ims, batch_size=BAT_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ResNet50(weights='imagenet') # using imagenet for now\n",
    "model2 = remove_softmax(model2)\n",
    "preds = get_cnn_output(model2, imlist[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measure - image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0    1    2    3    4         5    6    7         8    9    \\\n0     0.000000  0.0  0.0  0.0  0.0  4.511358  0.0  0.0  1.593750  0.0   \n1     0.000000  0.0  0.0  0.0  0.0  2.628688  0.0  0.0  2.861005  0.0   \n2     0.000000  0.0  0.0  0.0  0.0  4.633374  0.0  0.0  5.512821  0.0   \n3     0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  1.947205  0.0   \n4     0.000000  0.0  0.0  0.0  0.0  5.068770  0.0  0.0  8.384791  0.0   \n5     0.000000  0.0  0.0  0.0  0.0  2.664955  0.0  0.0  2.104283  0.0   \n6     0.000000  0.0  0.0  0.0  0.0  2.887506  0.0  0.0  5.711950  0.0   \n7     0.000000  0.0  0.0  0.0  0.0  4.311618  0.0  0.0  3.856278  0.0   \n8     0.000000  0.0  0.0  0.0  0.0  0.392628  0.0  0.0  2.259397  0.0   \n9     0.000000  0.0  0.0  0.0  0.0  3.131465  0.0  0.0  2.222769  0.0   \n10    0.000000  0.0  0.0  0.0  0.0  6.991385  0.0  0.0  5.414572  0.0   \n11    0.000000  0.0  0.0  0.0  0.0  1.737083  0.0  0.0  6.716125  0.0   \n12    0.000000  0.0  0.0  0.0  0.0  2.151319  0.0  0.0  0.497396  0.0   \n13    0.000000  0.0  0.0  0.0  0.0  5.621410  0.0  0.0  6.521851  0.0   \n14    0.000000  0.0  0.0  0.0  0.0  5.810854  0.0  0.0  4.591544  0.0   \n15    0.000000  0.0  0.0  0.0  0.0  4.410932  0.0  0.0  6.685376  0.0   \n16    0.000000  0.0  0.0  0.0  0.0  5.210143  0.0  0.0  5.193776  0.0   \n17    0.000000  0.0  0.0  0.0  0.0  2.469454  0.0  0.0  4.638887  0.0   \n18    0.000000  0.0  0.0  0.0  0.0  1.241429  0.0  0.0  2.495102  0.0   \n19    0.000000  0.0  0.0  0.0  0.0  3.362549  0.0  0.0  2.878283  0.0   \n20    0.000000  0.0  0.0  0.0  0.0  7.095930  0.0  0.0  5.637396  0.0   \n21    0.000000  0.0  0.0  0.0  0.0  2.049137  0.0  0.0  5.256789  0.0   \n22    0.000000  0.0  0.0  0.0  0.0  0.956707  0.0  0.0  1.259163  0.0   \n23    0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.804984  0.0   \n24    0.000000  0.0  0.0  0.0  0.0  4.160471  0.0  0.0  1.581592  0.0   \n25    0.000000  0.0  0.0  0.0  0.0  3.427929  0.0  0.0  3.915292  0.0   \n26    0.000000  0.0  0.0  0.0  0.0  3.524586  0.0  0.0  3.053069  0.0   \n27    0.000000  0.0  0.0  0.0  0.0  4.406858  0.0  0.0  2.088440  0.0   \n28    0.000000  0.0  0.0  0.0  0.0  2.927376  0.0  0.0  0.981968  0.0   \n29    0.000000  0.0  0.0  0.0  0.0  3.482943  0.0  0.0  1.451691  0.0   \n...        ...  ...  ...  ...  ...       ...  ...  ...       ...  ...   \n1907  0.000000  0.0  0.0  0.0  0.0  1.493038  0.0  0.0  2.553506  0.0   \n1908  0.000000  0.0  0.0  0.0  0.0  3.324339  0.0  0.0  2.842073  0.0   \n1909  0.000000  0.0  0.0  0.0  0.0  5.497582  0.0  0.0  3.048058  0.0   \n1910  0.000000  0.0  0.0  0.0  0.0  3.666061  0.0  0.0  1.829634  0.0   \n1911  0.000000  0.0  0.0  0.0  0.0  0.653075  0.0  0.0  3.262249  0.0   \n1912  0.000000  0.0  0.0  0.0  0.0  1.918170  0.0  0.0  0.000000  0.0   \n1913  0.000000  0.0  0.0  0.0  0.0  2.627792  0.0  0.0  2.589670  0.0   \n1914  0.162582  0.0  0.0  0.0  0.0  5.470705  0.0  0.0  4.488193  0.0   \n1915  0.000000  0.0  0.0  0.0  0.0  7.040302  0.0  0.0  5.233436  0.0   \n1916  0.000000  0.0  0.0  0.0  0.0  3.811166  0.0  0.0  5.488968  0.0   \n1917  0.000000  0.0  0.0  0.0  0.0  6.660167  0.0  0.0  3.341475  0.0   \n1918  0.000000  0.0  0.0  0.0  0.0  2.057449  0.0  0.0  2.767543  0.0   \n1919  0.000000  0.0  0.0  0.0  0.0  1.414272  0.0  0.0  2.557348  0.0   \n1920  0.000000  0.0  0.0  0.0  0.0  2.182818  0.0  0.0  1.646215  0.0   \n1921  0.000000  0.0  0.0  0.0  0.0  0.725296  0.0  0.0  0.893509  0.0   \n1922  0.000000  0.0  0.0  0.0  0.0  2.302182  0.0  0.0  2.512989  0.0   \n1923  0.000000  0.0  0.0  0.0  0.0  4.843776  0.0  0.0  3.752130  0.0   \n1924  0.000000  0.0  0.0  0.0  0.0  6.784534  0.0  0.0  5.793671  0.0   \n1925  0.000000  0.0  0.0  0.0  0.0  5.937938  0.0  0.0  2.972432  0.0   \n1926  0.000000  0.0  0.0  0.0  0.0  3.282385  0.0  0.0  2.267695  0.0   \n1927  0.000000  0.0  0.0  0.0  0.0  6.350508  0.0  0.0  3.514862  0.0   \n1928  0.000000  0.0  0.0  0.0  0.0  7.986290  0.0  0.0  6.720509  0.0   \n1929  0.000000  0.0  0.0  0.0  0.0  0.726039  0.0  0.0  3.619032  0.0   \n1930  0.000000  0.0  0.0  0.0  0.0  2.465192  0.0  0.0  3.698807  0.0   \n1931  0.000000  0.0  0.0  0.0  0.0  0.163951  0.0  0.0  0.000000  0.0   \n1932  0.000000  0.0  0.0  0.0  0.0  4.647576  0.0  0.0  6.006042  0.0   \n1933  0.000000  0.0  0.0  0.0  0.0  6.960378  0.0  0.0  5.413977  0.0   \n1934  0.000000  0.0  0.0  0.0  0.0  2.454787  0.0  0.0  2.532327  0.0   \n1935  0.000000  0.0  0.0  0.0  0.0  4.181670  0.0  0.0  3.706883  0.0   \n1936  0.000000  0.0  0.0  0.0  0.0  5.150801  0.0  0.0  3.958338  0.0   \n\n        ...     490  491  492  493       494       495       496  497  498  \\\n0       ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n1       ...     0.0  0.0  0.0  0.0  0.581597  0.000000  0.000000  0.0  0.0   \n2       ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.862850  0.0  0.0   \n3       ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n4       ...     0.0  0.0  0.0  0.0  0.000000  0.000000  1.353111  0.0  0.0   \n5       ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n6       ...     0.0  0.0  0.0  0.0  0.000000  0.000000  1.375753  0.0  0.0   \n7       ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n8       ...     0.0  0.0  0.0  0.0  0.000000  0.244845  0.000000  0.0  0.0   \n9       ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n10      ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n11      ...     0.0  0.0  0.0  0.0  3.246186  0.000000  0.408996  0.0  0.0   \n12      ...     0.0  0.0  0.0  0.0  1.133159  0.000000  0.000000  0.0  0.0   \n13      ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n14      ...     0.0  0.0  0.0  0.0  2.195986  0.000000  0.000000  0.0  0.0   \n15      ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n16      ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.981028  0.0  0.0   \n17      ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.434614  0.0  0.0   \n18      ...     0.0  0.0  0.0  0.0  2.381636  0.000000  1.672317  0.0  0.0   \n19      ...     0.0  0.0  0.0  0.0  0.000000  0.000000  1.304808  0.0  0.0   \n20      ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n21      ...     0.0  0.0  0.0  0.0  3.045341  0.000000  0.000000  0.0  0.0   \n22      ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n23      ...     0.0  0.0  0.0  0.0  2.120304  0.000000  0.000000  0.0  0.0   \n24      ...     0.0  0.0  0.0  0.0  0.640304  0.000000  0.000000  0.0  0.0   \n25      ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n26      ...     0.0  0.0  0.0  0.0  1.980268  0.000000  0.000000  0.0  0.0   \n27      ...     0.0  0.0  0.0  0.0  2.307639  0.000000  0.000000  0.0  0.0   \n28      ...     0.0  0.0  0.0  0.0  1.902931  0.000000  0.000000  0.0  0.0   \n29      ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n...     ...     ...  ...  ...  ...       ...       ...       ...  ...  ...   \n1907    ...     0.0  0.0  0.0  0.0  3.832903  0.000000  0.000000  0.0  0.0   \n1908    ...     0.0  0.0  0.0  0.0  1.544561  0.000000  0.000000  0.0  0.0   \n1909    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n1910    ...     0.0  0.0  0.0  0.0  1.211061  0.000000  0.000000  0.0  0.0   \n1911    ...     0.0  0.0  0.0  0.0  1.377769  0.000000  0.000000  0.0  0.0   \n1912    ...     0.0  0.0  0.0  0.0  0.000000  1.620504  0.000000  0.0  0.0   \n1913    ...     0.0  0.0  0.0  0.0  0.000000  2.205057  1.549631  0.0  0.0   \n1914    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  1.036694  0.0  0.0   \n1915    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n1916    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n1917    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n1918    ...     0.0  0.0  0.0  0.0  2.515833  0.000000  0.000000  0.0  0.0   \n1919    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n1920    ...     0.0  0.0  0.0  0.0  0.877173  0.635322  0.320592  0.0  0.0   \n1921    ...     0.0  0.0  0.0  0.0  0.000000  0.554112  0.000000  0.0  0.0   \n1922    ...     0.0  0.0  0.0  0.0  0.000000  1.215400  1.380178  0.0  0.0   \n1923    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.090691  0.0  0.0   \n1924    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n1925    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n1926    ...     0.0  0.0  0.0  0.0  2.682092  0.000000  0.000000  0.0  0.0   \n1927    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n1928    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n1929    ...     0.0  0.0  0.0  0.0  1.630681  0.000000  0.000000  0.0  0.0   \n1930    ...     0.0  0.0  0.0  0.0  0.417295  0.000000  0.000000  0.0  0.0   \n1931    ...     0.0  0.0  0.0  0.0  0.000000  1.432029  0.860641  0.0  0.0   \n1932    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.247454  0.0  0.0   \n1933    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0   \n1934    ...     0.0  0.0  0.0  0.0  0.259419  0.000000  0.173434  0.0  0.0   \n1935    ...     0.0  0.0  0.0  0.0  0.000000  0.000000  0.378052  0.0  0.0   \n1936    ...     0.0  0.0  0.0  0.0  0.286360  0.000000  0.000000  0.0  0.0   \n\n           499  \n0     1.484453  \n1     5.133023  \n2     0.000000  \n3     4.941018  \n4     2.317676  \n5     2.023387  \n6     2.620220  \n7     3.864774  \n8     6.369367  \n9     3.397144  \n10    2.700350  \n11    3.143169  \n12    3.059852  \n13    0.000000  \n14    4.368463  \n15    3.283463  \n16    2.123797  \n17    2.171373  \n18    0.010461  \n19    4.729648  \n20    1.424498  \n21    0.000000  \n22    1.915068  \n23    0.479282  \n24    2.270555  \n25    1.824338  \n26    5.475116  \n27    0.000000  \n28    1.422799  \n29    0.337428  \n...        ...  \n1907  4.034236  \n1908  5.717338  \n1909  2.247745  \n1910  1.960820  \n1911  1.040334  \n1912  6.016579  \n1913  4.109076  \n1914  2.573027  \n1915  2.698170  \n1916  2.948313  \n1917  2.323613  \n1918  0.589275  \n1919  4.479352  \n1920  4.358613  \n1921  4.070166  \n1922  4.842772  \n1923  0.984403  \n1924  1.455044  \n1925  2.637170  \n1926  3.585570  \n1927  1.159433  \n1928  6.184649  \n1929  1.133825  \n1930  3.186168  \n1931  3.903417  \n1932  3.478336  \n1933  0.418079  \n1934  4.142007  \n1935  5.310224  \n1936  1.962039  \n\n[1937 rows x 500 columns]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3f42b0682e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0muser_selected_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_k_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoogle_cnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_selected_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8e3b1f4635e6>\u001b[0m in \u001b[0;36mget_top_k_indices\u001b[0;34m(google_cnn_output, user_selected_ids, k)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0muser_selected_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_selected_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msimilarity_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_selected_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoogle_cnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_selected_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "PICKLED_FILE = 'google1937.p'\n",
    "\n",
    "with open(PICKLED_FILE, 'rb') as pickle_file:\n",
    "    content = pickle.load(pickle_file)\n",
    "\n",
    "google_cnn_output = pd.DataFrame(content)\n",
    "print(google_cnn_output)\n",
    "\n",
    "user_selected_ids = [1,33]\n",
    "\n",
    "output = get_top_k_indices(google_cnn_output, user_selected_ids, 5)\n",
    "print(output)\n",
    "                                 \n",
    "\n",
    "def get_top_k_indices(google_cnn_output, user_selected_ids, k):\n",
    "    \n",
    "    '''\n",
    "    NOTE: user_selected_imgs NEEDS TO BE A LIST! Even if it's only 1 item.\n",
    "    It does not handle 0 items at this moment.\n",
    "    \n",
    "    Both google_cnn_output and user_selected_imgs are output from the CNN and a np.array\n",
    "    '''\n",
    "    \n",
    "    user_selected_imgs = google_cnn_output.loc[user_selected_ids]\n",
    "    if len(google_cnn_output.shape) == 1:\n",
    "        google_cnn_output = google_cnn_output.reshape(1, -1)\n",
    "    if len(user_selected_imgs.shape) == 1:\n",
    "        user_selected_imgs = user_selected_imgs.reshape(1, -1)\n",
    "        \n",
    "    similarity_results = np.zeros((len(user_selected_imgs), len(google_cnn_output)))\n",
    "\n",
    "    for idx, img in enumerate(user_selected_imgs):\n",
    "        similarity_results[idx,:] = cosine_similarity(img.reshape(1, -1), google_cnn_output)\n",
    "        \n",
    "    print(similarity_results)\n",
    "        \n",
    "    if similarity_results.shape[0] == 1:\n",
    "        sorted_indices = np.argsort(similarity_results[0])\n",
    "    \n",
    "    else:\n",
    "        means = np.mean(similarity_results, axis=0)\n",
    "        sorted_indices = np.argsort(means)\n",
    "    \n",
    "    if k > len(google_cnn_output):\n",
    "        return (sorted_indices)\n",
    "    \n",
    "    top_indices = sorted_indices[-k:]\n",
    "\n",
    "    return(list(reversed(top_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99999964  0.79235113  0.74565804  0.75529718]\n",
      " [ 0.79235113  1.00000012  0.78746468  0.78237677]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 0, 3, 2]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_top_k_indices(preds, preds[0:2], 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
